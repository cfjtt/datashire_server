#SPARK_MASTER_URL=spark://e101:7077
#SPARK_HOME_DIR=/home/squid/dse/spark
#SCHEDULER_LOCATION=/home/squid/soft/spark_yarn/conf/fairscheduler.xml
# spark
SPARK_EXECUTOR_MEMORY=2g
SPARK_YARN_QUEUE=spark
SPARK_EXECUTOR_CORES=1
SPARK_EXECUTOR_INSTANCES=0
SPARK_DRIVER_MEMORY=2G
SPARK_YARN_ENGINE_JARS_DIR=hdfs://192.168.137.120:8020/user/datashire/spark2-ds-jars/
SPARK_JAR_LOCATION=engine.jar
SPARK_PROXY_USER=
SPARK_DRIVER_HOST=
#SPARK_JAR_LOCATION=/Users/zhudebin/Documents/iworkspace/ds_trunk/out/artifacts/engine_jar/engine.jar
#SPARK_PROXY_USER=squid
SPARK_CONFIG={"spark.executor.logs.rolling.maxRetainedFiles":"10", "spark.executor.logs.rolling.strategy":"size", "spark.executor.logs.rolling.maxSize":"1024000","spark.sql.crossJoin.enabled":"true","spark.default.parallelism":"20","spark.hadoop.cloneConf":"true"}
# 添加了时区设置-Duser.timezone=
spark_driver_extraJavaOptions=-Duser.timezone=Asia/Shanghai -XX:PermSize=128M -XX:MaxPermSize=512m
#spark_executor_extraJavaOptions=-XX:PermSize=128M -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/hdata/data01/
# 添加了时区设置-Duser.timezone=
spark_executor_extraJavaOptions=-Duser.timezone=Asia/Shanghai -XX:PermSize=128M -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/hdata/data01/

# 动态资源分配
SPARK_DYNAMICALLOCATION_ENABLED=true
SPARK_DYNAMICALLOCATION_CONFIG={"spark.shuffle.service.enabled":"true","spark.dynamicAllocation.cachedExecutorIdleTimeout":"1200s","spark.dynamicAllocation.minExecutors":"1"}

# 单作业最大运行task数
spark.datashire.scheduler.maxrunningtask=5

#最大并发运行数
PARALLEL_NUM=10
#hbase 默认分片数
HBASE_EXTRACT_PARTITIONS_DEFAULT=32
# 数据库分页查询，pageSize
SQL_PAGE_SIZE=10000

# engine rpc server 测试使用
ENGINE_RPC_SERVER_IP=127.0.0.1
#ENGINE_RPC_SERVER_IP=192.168.137.101
ENGINE_RPC_SERVER_PORT=11099

# report rpc server 报表功能已经移除
REPORT_RPC_SERVER_IP=127.0.0.1
REPORT_RPC_SERVER_PORT=9002

# server rpc server
SERVER_RPC_SERVER_IP=127.0.0.1
SERVER_RPC_SERVER_PORT=9003

# crawler rpc server爬虫功能已经移除
CRAWLER_RPC_SERVER_IP=127.0.0.1
CRAWLER_RPC_SERVER_PORT=9093

# KAFKA Brokers
KAFKA_BROKER_LIST=192.168.137.120:9092
KAFKA_LOG_TOPIC=ds_log

# zookeeper kafka
KAFKA_ZOOKEEPER_ADDRESS=192.168.137.120:2181

MAIL_PROTOCOL=smtp
MAIL_HOST=smtp.exmail.qq.com
MAIL_PORT=465
MAIL_USERNAME=ceshi@eurlanda.com
MAIL_PASSWORD=Ce123456
MAIL_SMTP_AUTH=true
MAIL_SMTP_STARTTLS_ENABLE=true

jdbc.pool.init=1
jdbc.pool.minIdle=3
jdbc.pool.maxActive=10
validationQuery.sql=SELECT 1

IS_LOG_TO_DB=false
IS_LOCAL=true
IS_START_SCHEDULE=false
START_THRIFTSERVER=false
ENABLE_HIVE=false

HIVE_SERVER2_THRIFT_PORT=10008

# webService
WEB_SERVICE.TOKEN=85C53BB8CAF87075DB8EFCB56CAB13D6
WEB_SERVICE.PORT=8580
WEB_SERVICE.URL=DataShireApi
WEB_SERVICE.IP=192.168.137.151

# 内置的 mysql 地址
INNER_MYSQL_HOST=192.168.137.176
INNER_MYSQL_PORT=3306
INNER_MYSQL_DATABASENAME=datashire_database
INNER_MYSQL_USERNAME=root
INNER_MYSQL_PASSWORD=111111

INNER_WEB_MYSQL_HOST=192.168.137.176
INNER_WEB_MYSQL_PORT=3306
INNER_WEB_MYSQL_DATABASENAME=datashire_cloud
INNER_WEB_MYSQL_USERNAME=root
INNER_WEB_MYSQL_PASSWORD=111111

# 内置的 DataMining  mysql地址
INNER_DataMining_MYSQL_DATABASENAME=datashire_DataMining

# cloud 新增配置
# 如果配置项  CLOUD_HDFS_FS_DEFAULTFS 这个配置了,则使用 CLOUD_HDFS_FS_DEFAULTFS
CLOUD_HDFS_FS_DEFAULTFS=192.168.137.183:8020
CLOUD_HDFS_PRIVATE_DATASHIRE_SPACE=__dsfileserverFS
CLOUD_HDFS_PUBLIC_DATASHIRE_SPACE=__dsfileserverFS
CLOUD_DB_NUM=1
CLOUD_DB_IP_PORT0=p1:3306
CLOUD_DB_IP_PORT1=p2:3306
CLOUD_DB_IP_PORT2=p3:3306
CLOUD_DB_PRIVATE_DATASHIRE__IP_PORT=__biwarehouse
CLOUD_DB_PUBLIC_DATASHIRE__IP_PORT=__biwarehouse

# 是否是公有数猎场
IS_CLOUD_PUBLIC_PROJECT=true

# 系统默认时区
SYSTEM_TIME_ZONE=GMT+8

#cassandra默认端口
CASSANDRA_PORT=9042

#任务最长等待时间(10分钟)
WAIT_TIME_OUT=1000*60*10

#队列长度
WAIT_QUEUE_SIZE=2000

#实训squid
train_file_host=__dsfileTrainFS
train_file_real_host=192.168.137.183:8020
#实训file
train_db_host=__dbTrainHouse
train_db_real_host=192.168.137.176:3306
